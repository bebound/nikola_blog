<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>KK's Blog · Models and Architechtures in Word2vec </title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://bebound.github.io/posts/models-and-architechtures-in-word2vec/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Kurt Lei">
<link rel="prev" href="../semi-supervised-text-classification-using-doc2vec-and-label-spreading/" title="Semi-supervised text classification using doc2vec and label spreading" type="text/html">
<link rel="next" href="../lstm-and-gru/" title="LSTM and GRU" type="text/html">
<meta property="og:site_name" content="KK's Blog">
<meta property="og:title" content="Models and Architechtures in Word2vec">
<meta property="og:url" content="https://bebound.github.io/posts/models-and-architechtures-in-word2vec/">
<meta property="og:description" content="Models
CBOW (Continuous Bag of Words)
Use the context to predict the probability of current word.


Context words' vectors are $\upsilon_{c-n} ... \upsilon_{c+m}$ ($m$ is the window size)
Context vect">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-01-05T21:57:17+08:00">
<meta property="article:tag" content="mathjax">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://bebound.github.io/">
                      <h1 id="brand"><a href="https://bebound.github.io/" title="KK's Blog" rel="home">

        <span id="blog-title">KK's Blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">KK's personal blog</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../archive.html">Archive</a>
        <a class="sidebar-nav-item" href="../../categories/">Tags</a>
        <a class="sidebar-nav-item" href="../../rss.xml">RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2018         <a href="mailto:bebound%20gm@il.com">Kurt Lei</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">Models and Architechtures in Word2vec</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2018-01-05T21:57:17+08:00" itemprop="datePublished" title="2018-01-05 21:57">2018-01-05 21:57</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h3>Models</h3>
<h4>CBOW (Continuous Bag of Words)</h4>
<p>Use the context to predict the probability of current word.</p>
<p><img alt="cbow" src="../../images/doc2vec_cbow.png"></p>
<ol>
<li>Context words' vectors are $\upsilon_{c-n} ... \upsilon_{c+m}$ ($m$ is the window size)</li>
<li>Context vector $ \hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+...+\upsilon_{c+m}}{2m} $</li>
<li>Score vector $z_i = u_i\hat{\upsilon}$, where $u_i$ is the output vector representation of word $\omega_i$</li>
<li>Turn scores into probabilities $\hat{y}=softmax(z)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<p>We use cross entropy $H(\hat{y},y)$ to measure the distance between these two distributions.
$$H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}$$</p>
<p>$y$ and $\hat{y}$ is accurate, so the loss simplifies to:
$$H(\hat{y},y)=-y_j\log(\hat{y})$$</p>
<p>For perfect prediction, $H(\hat{y},y)=-1\log(1)=0$</p>
<p>According to this, we can create this loss function:</p>
<div>

$$\begin{aligned}
minimize\ J &amp;=-\log P(\omega_c\lvert \omega_{c-m},...,\omega_{c-1},...,\omega_{c+m}) \\
&amp;= -\log P(u_c \lvert \hat{\upsilon}) \\
&amp;= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\
&amp;= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}$$

</div>

<h4>Skip-Gram</h4>
<p>Use current word to predict its context.</p>
<p><img alt="cbow" src="../../images/doc2vec_skip-gram.png"></p>
<ol>
<li>We get the input word's vector $\upsilon_c$</li>
<li>Generate $2m$ score vectors, $uc_{c-m},...,u_{c-1},...,u_{c+m}$.</li>
<li>Turn scores into probabilities $\hat{y}=softmax(u)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<div>

$$\begin{aligned}
minimize J &amp;=-\log P(\omega_{c-m},...,\omega_{c-1},\omega_{c+1},...\omega_{c+m}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\
&amp;=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}$$

</div>

<h3>Models</h3>
<p>Minimize $J$ is expensive, as the summation is over $\lvert V \rvert$. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.</p>
<h4>Hierarchical Softmax</h4>
<p>Encode words into a huffman tree, then each word has a Huffman code. The probability of it's probability $P(w\lvert Context(\omega))$ can change to choose the right path from root the che leaf node, each node is a binary classification. Suppose code $0$ is a possitive label, $1$ is negative label. If the probability of a possitive classification is 
$$
\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}
$$</p>
<p>Then the probability of negative classification is
$$
1-\sigma(X^T_\omega \theta)
$$</p>
<p><img alt="cbow" src="../../images/doc2vec_hierarchical_softmax.png">
足球's Huffman code is $1001$, then it's probability in each node are</p>
<div>

$$\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&amp;=1-\sigma(X^T_\omega \theta^\omega_1))\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&amp;=\sigma(X^T_\omega \theta^\omega_2))\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&amp;=\sigma(X^T_\omega \theta^\omega_3))\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&amp;=1-\sigma(X^T_\omega \theta^\omega_4))\\
\end{aligned}$$

</div>

<p>where $\theta$ is prarameter in the node.</p>
<p>The probability of the <code>足球</code> is the production of these equation.</p>
<p>Generally,</p>
<div>

$$p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})$$

</div>

<h4>Negative Sampling</h4>
<p>Choose some negitive sample, add the probability of the negative word into loss function. Maximize the positive words' probability and minimize the negitive words' probability.</p>
<p>Let $P(D=0 \lvert \omega,c)$ be the probability that $(\omega,c)$ did not come from the corpus data. Then the objective funtion will be</p>
<div>

$$\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)$$

</div>

<p>where $\theta$ is the parameters of the model($\upsilon$ and $u$).</p>
<p>Ref:</p>
<ul>
<li><a href="http://www.hankcs.com/nlp/word2vec.html">word2vec原理推导与代码分析</a></li>
<li><a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf">CS 224D: Deep Learning for NLP Lecture Notes: Part I</a></li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解（一）目录和前言</a></li>
</ul>
</div>
    </div>
    <aside class="postpromonav"><nav><p itemprop="keywords" class="tags">
      </p>

            <div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../semi-supervised-text-classification-using-doc2vec-and-label-spreading/" rel="prev" title="Semi-supervised text classification using doc2vec and label spreading">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
                <a href="../lstm-and-gru/" rel="next" title="LSTM and GRU">
Next post
              </a>
            </span>

        </div>

    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="kkblog-1",
            disqus_url="https://bebound.github.io/posts/models-and-architechtures-in-word2vec/",
        disqus_title="Models and Architechtures in Word2vec",
        disqus_identifier="cache/posts/models-and-architechtures-in-word2vec.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><script>var disqus_shortname="kkblog-1";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
    
        

</body>
</html>
