<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>KK's Blog · TextCNN with Pytorch and Torchtext on Colab </title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://blog.fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Kurt Lei">
<link rel="prev" href="../csrf-in-django/" title="CSRF in Django" type="text/html">
<meta property="og:site_name" content="KK's Blog">
<meta property="og:title" content="TextCNN with Pytorch and Torchtext on Colab">
<meta property="og:url" content="https://blog.fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/">
<meta property="og:description" content="PyTorch is a really powerful framework to build the machine learning models. Although some some features is missing when compared with TensorFlow (For example, the early stop fucntion, History to draw">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-12-02T16:46:02+08:00">
<meta property="article:tag" content="mathjax">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://blog.fromkk.com/">
                      <h1 id="brand"><a href="https://blog.fromkk.com/" title="KK's Blog" rel="home">

        <span id="blog-title">KK's Blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">KK's personal blog</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../archive.html">Archive</a>
        <a class="sidebar-nav-item" href="../../categories/">Tags</a>
        <a class="sidebar-nav-item" href="../../rss.xml">RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2018         <a href="mailto:bebound%20gm@il.com">Kurt Lei</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">TextCNN with Pytorch and Torchtext on Colab</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2018-12-02T16:46:02+08:00" itemprop="datePublished" title="2018-12-02 16:46">2018-12-02 16:46</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <p>
<a href="https://pytorch.org">PyTorch</a> is a really powerful framework to build the machine learning models. Although some some features is missing when compared with TensorFlow (For example, the early stop fucntion, History to draw plot), its code style is more intuitive. 
</p>

<p>
<a href="https://github.com/pytorch/text">Torchtext</a> is a NLP package which is also made by <code>=pytorch=</code> team. It provide a way to read text, processing and iterate the texts.
</p>

<p>
<a href="https://colab.research.google.com">Google Colab</a> is a Jypyter notebook environment host by Google, you can use free GPU and TPU to run your modal.
</p>

<p>
Here is a simple tuturial to build a TextCNN modal and run it on Colab.
</p>

<p>
The <a href="https://arxiv.org/abs/1408.5882">TextCNN paper</a> was published by Kim in 2014. The model's idea is pretty simple, but the performance is impressive. If you trying to solve the text classificaton problem, this model is a good choice to start with.
</p>

<p>
The main architechture is shown below:
</p>

<p>
<img src="../../images/textcnn.png" alt="nil"></p>

<p>
It uses different kenels to extract text features, then use the softmax regerission to classify text base on the featrues.
</p>

<p>
Now we can build this model step by step.
</p>

<p>
First build the model. The model I use is CNN-multichannel, which contains two sets of word emmbedding. Both of them is the copy of word embedding generate from corpus, but only one set will update embedding during training.
</p>

<p>
The code is below:
</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">textCNNMulti</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">args</span><span class="p">):</span>
	<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
	<span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s1">'dim'</span><span class="p">]</span>
	<span class="n">n_class</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s1">'n_class'</span><span class="p">]</span>
	<span class="n">embedding_matrix</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'embedding_matrix'</span><span class="p">]</span>
	<span class="n">kernels</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
	<span class="n">kernel_number</span><span class="o">=</span><span class="p">[</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">]</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">static_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">)</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">non_static_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">number</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">number</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kernels</span><span class="p">,</span><span class="n">kernel_number</span><span class="p">)])</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">kernel_number</span><span class="p">),</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
	<span class="n">non_static_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_static_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">static_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">non_static_input</span><span class="p">,</span> <span class="n">static_input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">]</span>
	<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool1d</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">x</span>
</pre></div>

<p>
Second, convert text into word index, so each sentence become a vector for training.
</p>

<div class="highlight"><pre><span></span><span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">SST</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">,</span> <span class="s1">'data/'</span><span class="p">,</span><span class="n">fine_grained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="s2">"glove.840B.300d"</span><span class="p">)</span>
<span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">test</span><span class="p">)</span>

<span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>
    <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">),</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>

<p>
<code>=Field=</code> defines how to process text, here is the most common parameters:
</p>

<blockquote>
<p>
sequential – Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.
</p>

<p>
use_vocab – Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.
</p>

<p>
preprocessing – The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.
</p>

<p>
batch_first – Whether to produce tensors with the batch dimension first. Default: False.
</p>
</blockquote>

<p>
<code>=datasets.SST.splits=</code> will load the <code>=SST=</code> datasets, and split into train, validation, and test Dataset objects.
</p>

<p>
<code>=build_vocab=</code> will create the Vocab object for Field, which contains the information to convert word into word index and vice versa. Also, the word embedding will save as <code>=Field.Vocab.vectors=</code>. <code>=vectors=</code> contains all of the word embedding. Torchtext can download some pretrained vectors automaticaly, such as <code>=glove.840B.300d=</code>, <code>=fasttext.en.300d=</code>. You can also load your vectors in this way, <code>=xxx.vec=</code> should be the standard word2vec format.
</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">Vectors</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">Vectors</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'xxx.vec'</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="s1">'./'</span><span class="p">)</span>
<span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="n">vectors</span><span class="p">)</span>
</pre></div>

<p>
<code>=data.BucketIterator.splits=</code> will returns iterators that loads batches of data from datasets, and the text in same batch has similar lengths.
</p>


<p>
Now, we can start to train the model. First we wrap some parameters into <code>=args=</code>, it contains settings like output class, learning rate, log inverval and so on.
</p>

<div class="highlight"><pre><span></span><span class="n">args</span><span class="o">=</span><span class="p">{}</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'vocb_size'</span><span class="p">]</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'dim'</span><span class="p">]</span><span class="o">=</span><span class="mi">300</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'n_class'</span><span class="p">]</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">LABEL</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'embedding_matrix'</span><span class="p">]</span><span class="o">=</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span><span class="o">=</span><span class="mf">0.001</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span><span class="o">=</span><span class="mf">0.8</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'epochs'</span><span class="p">]</span><span class="o">=</span><span class="mi">180</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'log_interval'</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'test_interval'</span><span class="p">]</span><span class="o">=</span><span class="mi">500</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'save_dir'</span><span class="p">]</span><span class="o">=</span><span class="s1">'./'</span>
</pre></div>


<p>
Finally, we can train the model.
</p>

<div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">textCNNMulti</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">],</span><span class="n">momentum</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">])</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">steps</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s1">'epochs'</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
	<span class="n">steps</span><span class="o">+=</span><span class="mi">1</span>

	<span class="n">x</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">label</span>
	<span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

	<span class="n">target</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

	<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
	<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
	<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
	<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<p>
Here is the full colab file: <a href="https://colab.research.google.com/drive/1iZE5O0aBEOEhkWNpARqK5u151qrlwJq-">textcnn.ipynb</a> 
</p>


<p>
Ref:
</p>

<ol class="org-ol">
<li>
<a href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classiﬁcation</a>
</li>
<li>
<a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">Understanding Convolutional Neural Networks for NLP</a>
</li>
<li>
<a href="https://torchtext.readthedocs.io/en/latest/data.html">Torchtext Docs</a>
</li>
<li>
<a href="https://github.com/castorini/Castor">Castor</a>
</li>
</ol>
</div>
    <aside class="postpromonav"><nav><p itemprop="keywords" class="tags">
      </p>

            <div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../csrf-in-django/" rel="prev" title="CSRF in Django">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
Next post
            </span>

        </div>

    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="kkblog-1",
            disqus_url="https://blog.fromkk.com/posts/textcnn-with-pytorch-and-torchtext-on-colab/",
        disqus_title="TextCNN with Pytorch and Torchtext on Colab",
        disqus_identifier="cache/posts/textcnn-with-pytorch-and-torchtext-on-colab.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><script>var disqus_shortname="kkblog-1";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
    
        

</body>
</html>
