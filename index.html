<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="KK's personal blog">
<meta name="viewport" content="width=device-width">
<title>KK's Blog</title>
<link href="assets/css/all.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://bebound.github.io/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/lstm-and-gru/" type="text/html">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://bebound.github.io/">
                      <h1 id="brand"><a href="https://bebound.github.io/" title="KK's Blog" rel="home">

        <span id="blog-title">KK's Blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">KK's personal blog</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="archive.html">Archive</a>
        <a class="sidebar-nav-item" href="categories/">Tags</a>
        <a class="sidebar-nav-item" href="rss.xml">RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2018         <a href="mailto:bebound%20gm@il.com">Kurt Lei</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/lstm-and-gru/" class="u-url">LSTM and GRU</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-04-22T23:27:45+08:00" title="2018-04-22 23:27">2018-04-22 23:27</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>LSTM</h3>
<p>The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.</p>
<p>Here is the structure of LSTM:</p>
<p><img alt="LSTM" src="images/LSTM_LSTM.png"></p>
<p>The calculate procedure are:</p>
<div>

$$
\begin{aligned}
f_t&amp;=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\
i_t&amp;=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\
o_t&amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\
\tilde{C_t}&amp;=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\
C_t&amp;=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\
h_t&amp;=o_t \ast tanh(C_t)
\end{aligned}
$$

</div>

<p>$f_t$,$i_t$,$o_t$ are forget gate, input gate and output gate respectively. $\tilde{C_t}$ is the new memory content. $Ct$ is cell state. $ht$ is the output. </p>
<p>Use $f_t$ and $i_t$ to update $C_t$, use $o_t$ to decide which part of hidden state should be outputted.</p>
<h3>GRU</h3>
<p><img alt="LSTM" src="images/LSTM_GRU.png"></p>
<div>

$$
\begin{aligned}
z_t&amp;=\sigma(W_z\cdot[h_{t-1},x_t])\\
r_t&amp;=\sigma(W_r\cdot[h_{t-1},x_t])\\
\tilde{h_t}&amp;=tanh(W\cdot[r_t \ast h_{t-1},x_t])\\
h_t&amp;=(1-z_t)\ast h_{t-1}+z_t \ast \tilde{h_t}
\end{aligned}
$$

</div>

<p>$z_t$ is update gate, $r_t$ is reset gate, $\tilde{h_t}$ is candidate activation, $h_t$ is activation.</p>
<p>Compare with LSTM, GRU merge cell state and hidden state to one hiddent state, and use $z_t$ to decide how to update the state rather than $f_t$ and $i_t$.</p>
<p>Ref：</p>
<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a href="https://blog.csdn.net/zhangxb35/article/details/70060295">RNN, LSTM, GRU 公式总结</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/models-and-architechtures-in-word2vec/" class="u-url">Models and Architechtures in Word2vec</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-01-05T21:57:17+08:00" title="2018-01-05 21:57">2018-01-05 21:57</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Models</h3>
<h4>CBOW (Continuous Bag of Words)</h4>
<p>Use the context to predict the probability of current word.</p>
<p><img alt="cbow" src="images/doc2vec_cbow.png"></p>
<ol>
<li>Context words' vectors are $\upsilon_{c-n} ... \upsilon_{c+m}$ ($m$ is the window size)</li>
<li>Context vector $ \hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+...+\upsilon_{c+m}}{2m} $</li>
<li>Score vector $z_i = u_i\hat{\upsilon}$, where $u_i$ is the output vector representation of word $\omega_i$</li>
<li>Turn scores into probabilities $\hat{y}=softmax(z)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<p>We use cross entropy $H(\hat{y},y)$ to measure the distance between these two distributions.
$$H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}$$</p>
<p>$y$ and $\hat{y}$ is accurate, so the loss simplifies to:
$$H(\hat{y},y)=-y_j\log(\hat{y})$$</p>
<p>For perfect prediction, $H(\hat{y},y)=-1\log(1)=0$</p>
<p>According to this, we can create this loss function:</p>
<div>

$$\begin{aligned}
minimize\ J &amp;=-\log P(\omega_c\lvert \omega_{c-m},...,\omega_{c-1},...,\omega_{c+m}) \\
&amp;= -\log P(u_c \lvert \hat{\upsilon}) \\
&amp;= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\
&amp;= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}$$

</div>

<h4>Skip-Gram</h4>
<p>Use current word to predict its context.</p>
<p><img alt="cbow" src="images/doc2vec_skip-gram.png"></p>
<ol>
<li>We get the input word's vector $\upsilon_c$</li>
<li>Generate $2m$ score vectors, $uc_{c-m},...,u_{c-1},...,u_{c+m}$.</li>
<li>Turn scores into probabilities $\hat{y}=softmax(u)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<div>

$$\begin{aligned}
minimize J &amp;=-\log P(\omega_{c-m},...,\omega_{c-1},\omega_{c+1},...\omega_{c+m}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\
&amp;=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}$$

</div>

<h3>Models</h3>
<p>Minimize $J$ is expensive, as the summation is over $\lvert V \rvert$. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.</p>
<h4>Hierarchical Softmax</h4>
<p>Encode words into a huffman tree, then each word has a Huffman code. The probability of it's probability $P(w\lvert Context(\omega))$ can change to choose the right path from root the che leaf node, each node is a binary classification. Suppose code $0$ is a possitive label, $1$ is negative label. If the probability of a possitive classification is 
$$
\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}
$$</p>
<p>Then the probability of negative classification is
$$
1-\sigma(X^T_\omega \theta)
$$</p>
<p><img alt="cbow" src="images/doc2vec_hierarchical_softmax.png">
足球's Huffman code is $1001$, then it's probability in each node are</p>
<div>

$$\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&amp;=1-\sigma(X^T_\omega \theta^\omega_1))\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&amp;=\sigma(X^T_\omega \theta^\omega_2))\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&amp;=\sigma(X^T_\omega \theta^\omega_3))\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&amp;=1-\sigma(X^T_\omega \theta^\omega_4))\\
\end{aligned}$$

</div>

<p>where $\theta$ is prarameter in the node.</p>
<p>The probability of the <code>足球</code> is the production of these equation.</p>
<p>Generally,</p>
<div>

$$p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})$$

</div>

<h4>Negative Sampling</h4>
<p>Choose some negitive sample, add the probability of the negative word into loss function. Maximize the positive words' probability and minimize the negitive words' probability.</p>
<p>Let $P(D=0 \lvert \omega,c)$ be the probability that $(\omega,c)$ did not come from the corpus data. Then the objective funtion will be</p>
<div>

$$\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)$$

</div>

<p>where $\theta$ is the parameters of the model($\upsilon$ and $u$).</p>
<p>Ref:</p>
<ul>
<li><a href="http://www.hankcs.com/nlp/word2vec.html">word2vec原理推导与代码分析</a></li>
<li><a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf">CS 224D: Deep Learning for NLP Lecture Notes: Part I</a></li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec 中的数学原理详解（一）目录和前言</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/" class="u-url">Semi-supervised text classification using doc2vec and label spreading</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-09-10T22:18:15+08:00" title="2017-09-10 22:18">2017-09-10 22:18</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Here is a simple way to classify text without much human effort and get a impressive performance.</p>
<p>It can be divided into two steps:</p>
<ol>
<li>Get train data by using keyword classificaton</li>
<li>Generate a more accurate classification model by using doc2vec and label spreading</li>
</ol>
<h4>Keyword-based Classification</h4>
<p>Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candicate.</p>
<ol>
<li>Find some most common words to classify the text.</li>
<li>Use this equition to calculate the score of each word appears in the text.
   $$ score(i) = \frac{count(i)}{all\_count(i)^{0.3}}$$
   which $all\_count(i)$ is the word i's wordc ount in all corpus, and $count(i)$ is the word i's word count in positive corpus.</li>
<li>Check the top words, add it to the final keyword list. Repeat this process.</li>
</ol>
<p>Finally, we can use the keywords to classify the text and get the train data. </p>
<h4>Classification by <code>doc2vec</code> and Label Spreading</h4>
<p>Keyword-based classification sometimes produces the wrong result, as it can't using the symantic information in the text. Fortunately, Google has open sourced <code>word2vec</code>, which can be used to produce semantically meaningful word embeddings. Furthermore, sentences can also be converted to vectors by using <code>doc2vec</code>. Sentences which has closed meaning also have short vector distance.</p>
<p>So the problem is how to classify these vectors.</p>
<ol>
<li>Using corpus to train the <code>doc2vec</code> model.</li>
<li>Using <code>doc2vec</code> model to convert sentence into vector.</li>
<li>Using label spreading algorithm to train a classify model to classify the vectors.</li>
</ol>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/parameters-in-dov2vec/" class="u-url">Parameters in dov2vec</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-08-03T00:02:17+08:00" title="2017-08-03 00:02">2017-08-03 00:02</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Here are some parameter in <code>gensim</code>'s <code>doc2vec</code> class.</p>
<h4>window</h4>
<p>window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.</p>
<p>In <code>skip-gram</code> model, if the window size is 2, the training samples will be this:(the blue word is the input word)</p>
<p><img alt="window" src="images/doc2vec_window.png"></p>
<h4>min_count</h4>
<p>If the word appears less than this value, it will be skipped</p>
<h4>sample</h4>
<p>High frequency word like <code>the</code> is useless for training. <code>sample</code> is a threshold for deleting these higher-frequency words. The probability of keeping the word $w_i$ is:</p>
<p>$$P(w_i) = (\sqrt{\frac{z(\omega_i)}{s}} + 1) \cdot \frac{s}{z(\omega_i)}$$</p>
<p>where $z(w_i)$ is the frequency of the word and $s$ is the sample rate.</p>
<p>This is the plot when <code>sample</code> is 1e-3.</p>
<p><img alt="negative-sample" src="images/doc2vec_negative_sample.png"></p>
<h4>negative</h4>
<p>Usually, when training a neural network, for each training sample, all of the weights in the neural network need to be tweaked. For example, if the word pair is ('fox', 'quick'), then only the word quick's neurons should output 1, and all of the other word neurons should output 0.</p>
<p>But it would takes a lot of time to do this when we have billions of training samples. So, instead of update all of the weight, we random choose a small number of "negative" words (default value is 5) to update the weight.(Update their wight to output 0).</p>
<p>So when dealing with word pair ('fox','quick'), we update quick's weight to output 1, and other 5 random words' wight to output 1.</p>
<p>The probability of selecting word $\omega_i$ is $P(\omega_i)$:</p>
<p>$$P(\omega_i) = \frac{  {f(\omega_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(\omega_j)}^{3/4} \right) }$$</p>
<p>$f(\omega_j)$ is the frequency of word $\omega_j$.</p>
<p>Ref:</p>
<ul>
<li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></li>
<li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">Word2Vec Tutorial Part 2 - Negative Sampling</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/brief-introduction-of-label-propagation-algorithm/" class="u-url">Brief Introduction of Label Propagation Algorithm</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-07-16T23:46:04+08:00" title="2017-07-16 23:46">2017-07-16 23:46</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>As I said before, I'm working on a text classification project. I use <code>doc2vec</code> to convert text into vectors, then I use LPA to classify the vectors.</p>
<p>LPA is a simple, effective semi-supervised algotithm. It can use the density of unlabeled data to find a hyperplane to split the data.</p>
<p>Here are the main stop of the algorithm:</p>
<ol>
<li>Let $ (x_1,y1)...(x_l,y_l)$ be labeled data, $Y_L = {y_1...y_l} $ are the class labels. Let $(x_{l+1},y_{l+u})$ be unlabeled data where $Y_U = {y_{l+1}...y_{l+u}}$ are unobserved, useally $l \ll u$. Let $X={x_1...x_{l+u}}$ where $x_i\in R^D$. The problem is to estimate $Y_U$ for $X$ and $Y_L$.</li>
<li>Calculate the similarity of the data points. The most simple metric is Euclidean distance. Use a parameter $\sigma$ to cotrol the weights.</li>
</ol>
<p>$$w_{ij}= exp(-\frac{d^2_{ij}}{\sigma^2})=exp(-\frac{\sum^D_{d=1}{(x^d_i-x^d_j})^2}{\sigma^2})$$</p>
<p>Larger weight allow labels to travel through easier.</p>
<ol>
<li>Define a $(l+u)*(l+u)$ probabilistic transition matrix $T$</li>
</ol>
<div>

$$T_{ij}=P(j \rightarrow i)=\frac{w_{ij}}{\sum^{l+u}_{k=1}w_{kj}}$$

</div>

<p>$T_{ij}$ is the probability to jump from node $j$ to $i$. If there are $C$ classes, we can define a $(l+u)*C$ label matrix $Y$, to represent the probability of a label belong to class $c$. The initialiation of unlabeled data points is not important.</p>
<ol>
<li>Propagate $Y \leftarrow TY$</li>
<li>Row-normalize Y.</li>
<li>Reset labeled data's Y. Repeat 3 until Y converges.</li>
</ol>
<p>In short, let the nearest label has larger weight, then calculate each label's new label, reset labeled data's label, repeat.</p>
<p><img alt="label spreading" src="images/label_spreading.png"></p>
<p>Ref:</p>
<ol>
<li>
<p><a href="http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf">Learning from Labeled and Unlabeled Data with Label Propagation</a></p>
</li>
<li>
<p><a href="http://blog.csdn.net/zouxy09/article/details/49105265">标签传播算法（Label Propagation）及Python实现</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/enable-c-extension-for-gensim-on-windows/" class="u-url">Enable C Extension for gensim on Windows</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-06-10T03:40:32+08:00" title="2017-06-10 03:40">2017-06-10 03:40</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>For these days, I’m working on some text classification works, and I use <code>gensim</code>’s <code>doc2vec</code> function.</p>
<p>When using gensim, it shows this warning message:</p>
<pre class="code literal-block"><span></span>C extension not loaded for Word2Vec, training will be slow.
</pre>


<p>I search this on Internet and found that gensim has rewrite some part of the code using <code>cython</code> rather than <code>numpy</code> to get better performance. A compiler is required to enable this feature.</p>
<p>I tried to install mingw and add it into the path, but it's not working.</p>
<p>Finally, I tried to install <a href="http://landinghub.visualstudio.com/visual-cpp-build-tools">Visual C++ Build Tools</a> and it works.</p>
<p>If this output a none <code>-1</code> digit, then it's fine.</p>
<pre class="code literal-block"><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="k">import</span> <span class="n">word2vec</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">FAST_VERSION</span><span class="p">)</span>
</pre>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/some-useful-shell-tools/" class="u-url">Some Useful Shell Tools</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-05-07T00:10:25+08:00" title="2017-05-07 00:10">2017-05-07 00:10</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Here are some shell tools I use, which can boost your productivity.</p>
<h4><a href="https://github.com/sorin-ionescu/prezto">Prezto</a></h4>
<p>A zsh configuration framework. Provides auto completion, prompt theme and lots of modules to work with other useful tools. I extremely love the <code>agnoster</code> theme.</p>
<p><img alt="agnoster" src="images/shell_agnoster.png"></p>
<h4><a href="https://github.com/clvv/fasd">Fasd</a></h4>
<p>Help you to navigate between folders and launch application.</p>
<p>Here are the official usage example:</p>
<pre class="code literal-block"><span></span>  v def conf       =&gt;     vim /some/awkward/path/to/type/default.conf
  j abc            =&gt;     cd /hell/of/a/awkward/path/to/get/to/abcdef
  m movie          =&gt;     mplayer /whatever/whatever/whatever/awesome_movie.mp4
  o eng paper      =&gt;     xdg-open /you/dont/remember/where/english_paper.pdf
  vim `f rc lo`    =&gt;     vim /etc/rc.local
  vim `f rc conf`  =&gt;     vim /etc/rc.conf
</pre>


<h4><a href="https://github.com/monochromegane/the_platinum_searcher">pt</a></h4>
<p>A fast code search tool similar to <code>ack</code>.</p>
<h4><a href="https://github.com/junegunn/fzf">fzf</a></h4>
<p>A great fuzzy finder, it can also integrate with vim by <a href="https://github.com/junegunn/fzf.vim">fzf.vim</a></p>
<p><img alt="fzf" src="images/shell_fzf.gif"></p>
<h4><a href="https://github.com/nvbn/thefuck">thefuck</a></h4>
<p>Magnificent app which corrects your previous console command.</p>
<p><img alt="thefuck" src="images/shell_thefuck.gif"></p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/start/" class="u-url">Start</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-18T23:33:55+08:00" title="2017-04-18 23:33">2017-04-18 23:33</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Over the years, I have read so many programmers’ blogs, which has  helped me a lot. Now I think it’s the time to start my own blog.</p>
<p>I hope this can enforce myself to review what I have learned, and it would even be better if someone can benefit from it.</p>
</div>
    </div>
    </article>
</div>

               <script>var disqus_shortname="kkblog-1";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script>
</div>

    
    
        

</body>
</html>
