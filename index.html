<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="KK's personal blog">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>KK's Blog</title>
<link href="assets/css/all.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://blog.fromkk.com/">
<link rel="next" href="index-1.html" type="text/html">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/textcnn-with-pytorch-and-torchtext-on-colab/" type="text/html">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://blog.fromkk.com/">
                      <h1 id="brand"><a href="https://blog.fromkk.com/" title="KK's Blog" rel="home">

        <span id="blog-title">KK's Blog</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">KK's personal blog</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="archive.html">Archive</a>
        <a class="sidebar-nav-item" href="categories/">Tags</a>
        <a class="sidebar-nav-item" href="rss.xml">RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents Â© 2019         <a href="mailto:bebound%20gm@il.com">Kurt Lei</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/textcnn-with-pytorch-and-torchtext-on-colab/" class="u-url">TextCNN with Pytorch and Torchtext on Colab</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-12-02T16:46:02+08:00" title="2018-12-02 16:46">2018-12-02 16:46</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>
<a href="https://pytorch.org">PyTorch</a> is a really powerful framework to build the machine learning models. Although some some features is missing when compared with TensorFlow (For example, the early stop fucntion, History to draw plot), its code style is more intuitive. 
</p>

<p>
<a href="https://github.com/pytorch/text">Torchtext</a> is a NLP package which is also made by <code>=pytorch=</code> team. It provide a way to read text, processing and iterate the texts.
</p>

<p>
<a href="https://colab.research.google.com">Google Colab</a> is a Jypyter notebook environment host by Google, you can use free GPU and TPU to run your modal.
</p>

<p>
Here is a simple tuturial to build a TextCNN modal and run it on Colab.
</p>

<p>
The <a href="https://arxiv.org/abs/1408.5882">TextCNN paper</a> was published by Kim in 2014. The model's idea is pretty simple, but the performance is impressive. If you trying to solve the text classificaton problem, this model is a good choice to start with.
</p>

<p>
The main architechture is shown below:
</p>

<p>
<img src="images/textcnn.png" alt="nil"></p>

<p>
It uses different kenels to extract text features, then use the softmax regerission to classify text base on the featrues.
</p>

<p>
Now we can build this model step by step.
</p>

<p>
First build the model. The model I use is CNN-multichannel, which contains two sets of word emmbedding. Both of them is the copy of word embedding generate from corpus, but only one set will update embedding during training.
</p>

<p>
The code is below:
</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">textCNNMulti</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">args</span><span class="p">):</span>
	<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
	<span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s1">'dim'</span><span class="p">]</span>
	<span class="n">n_class</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s1">'n_class'</span><span class="p">]</span>
	<span class="n">embedding_matrix</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'embedding_matrix'</span><span class="p">]</span>
	<span class="n">kernels</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
	<span class="n">kernel_number</span><span class="o">=</span><span class="p">[</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">]</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">static_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">)</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">non_static_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">number</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">number</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kernels</span><span class="p">,</span><span class="n">kernel_number</span><span class="p">)])</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">()</span>
	<span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">kernel_number</span><span class="p">),</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
	<span class="n">non_static_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_static_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">static_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">static_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">non_static_input</span><span class="p">,</span> <span class="n">static_input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">]</span>
	<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool1d</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">x</span>
</pre></div>

<p>
Second, convert text into word index, so each sentence become a vector for training.
</p>

<div class="highlight"><pre><span></span><span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">SST</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">,</span> <span class="s1">'data/'</span><span class="p">,</span><span class="n">fine_grained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="s2">"glove.840B.300d"</span><span class="p">)</span>
<span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">val</span><span class="p">,</span><span class="n">test</span><span class="p">)</span>

<span class="n">train_iter</span><span class="p">,</span> <span class="n">val_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span>
    <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">),</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>

<p>
<code>=Field=</code> defines how to process text, here is the most common parameters:
</p>

<blockquote>
<p>
sequential â Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.
</p>

<p>
use_vocab â Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.
</p>

<p>
preprocessing â The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.
</p>

<p>
batch_first â Whether to produce tensors with the batch dimension first. Default: False.
</p>
</blockquote>

<p>
<code>=datasets.SST.splits=</code> will load the <code>=SST=</code> datasets, and split into train, validation, and test Dataset objects.
</p>

<p>
<code>=build_vocab=</code> will create the Vocab object for Field, which contains the information to convert word into word index and vice versa. Also, the word embedding will save as <code>=Field.Vocab.vectors=</code>. <code>=vectors=</code> contains all of the word embedding. Torchtext can download some pretrained vectors automaticaly, such as <code>=glove.840B.300d=</code>, <code>=fasttext.en.300d=</code>. You can also load your vectors in this way, <code>=xxx.vec=</code> should be the standard word2vec format.
</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">Vectors</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">Vectors</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'xxx.vec'</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="s1">'./'</span><span class="p">)</span>
<span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="n">vectors</span><span class="p">)</span>
</pre></div>

<p>
<code>=data.BucketIterator.splits=</code> will returns iterators that loads batches of data from datasets, and the text in same batch has similar lengths.
</p>


<p>
Now, we can start to train the model. First we wrap some parameters into <code>=args=</code>, it contains settings like output class, learning rate, log inverval and so on.
</p>

<div class="highlight"><pre><span></span><span class="n">args</span><span class="o">=</span><span class="p">{}</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'vocb_size'</span><span class="p">]</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'dim'</span><span class="p">]</span><span class="o">=</span><span class="mi">300</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'n_class'</span><span class="p">]</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">LABEL</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'embedding_matrix'</span><span class="p">]</span><span class="o">=</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">]</span><span class="o">=</span><span class="mf">0.001</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">]</span><span class="o">=</span><span class="mf">0.8</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'epochs'</span><span class="p">]</span><span class="o">=</span><span class="mi">180</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'log_interval'</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'test_interval'</span><span class="p">]</span><span class="o">=</span><span class="mi">500</span>
<span class="n">args</span><span class="p">[</span><span class="s1">'save_dir'</span><span class="p">]</span><span class="o">=</span><span class="s1">'./'</span>
</pre></div>


<p>
Finally, we can train the model.
</p>

<div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">textCNNMulti</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'lr'</span><span class="p">],</span><span class="n">momentum</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s1">'momentum'</span><span class="p">])</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">steps</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s1">'epochs'</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
	<span class="n">steps</span><span class="o">+=</span><span class="mi">1</span>

	<span class="n">x</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">label</span>
	<span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

	<span class="n">target</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

	<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
	<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
	<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
	<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>


<p>
Here is the full colab file: <a href="https://colab.research.google.com/drive/1iZE5O0aBEOEhkWNpARqK5u151qrlwJq-">textcnn.ipynb</a> 
</p>


<p>
Ref:
</p>

<ol class="org-ol">
<li>
<a href="https://arxiv.org/abs/1408.5882">Convolutional Neural Networks for Sentence Classiï¬cation</a>
</li>
<li>
<a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">Understanding Convolutional Neural Networks for NLP</a>
</li>
<li>
<a href="https://torchtext.readthedocs.io/en/latest/data.html">Torchtext Docs</a>
</li>
<li>
<a href="https://github.com/castorini/Castor">Castor</a>
</li>
</ol>
</div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/csrf-in-django/" class="u-url">CSRF in Django</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-11-07T23:12:14+08:00" title="2018-11-07 23:12">2018-11-07 23:12</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>
CSRF(Cross-site request forgery) is a way to generate fake user request to target website. For example, on a malicious website A, there is a button, click it will send request to www.B.com/logout. When the user click this button, he will logout from website B unconsciously. Logout is not a big problem, but malicious website can generate more dangerous request like money transfer.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Django CSRF protection</h2>
<div class="outline-text-2" id="text-1">
<p>
Each web framework has different approach to do CSRF protection. In Django, the  validation process is below:
</p>

<ol class="org-ol">
<li>When user login for the first time, Django generate a <code>=csrf_secret=</code>, add random salt and encrypt it as A, save A to cookie <code>=csrftoken=</code>.
</li>
<li>When Django processing tag <code>={{ csrf_token }}=</code> or <code>={% csrf_token %}=</code>, it read <code>=csrftoken=</code> cookie A, reverse it to <code>=csrf_secret=</code>, add random salt and encrypt it as B, return corresponding HTML.
</li>
<li>When Django receive POST request, it will retrive cookie <code>=csrftoken=</code> as A, and tries to get <code>=csrfmiddlewaretoken=</code> value B from POST data, if it does not exist, it will get header <code>=X-CSRFToken=</code> value as B. Then A and B will be reversed to <code>=csrf_secret=</code>. If the values are identical, the validation is passed. Otherwise, a 403 error will raise.
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Django CSRF Usage</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Form</h3>
<div class="outline-text-3" id="text-2-1">
<div class="highlight"><pre><span></span>&lt;form&gt;
    {% csrf_token %}
&lt;/form&gt;
</pre></div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Single AJAX request</h3>
<div class="outline-text-3" id="text-2-2">
<div class="highlight"><pre><span></span><span class="nx">$</span><span class="p">.</span><span class="nx">ajax</span><span class="p">({</span>
    <span class="nx">data</span><span class="o">:</span> <span class="p">{</span>
	<span class="nx">csrfmiddlewaretoken</span><span class="o">:</span> <span class="s1">'{{ csrf_token }}'</span>
    <span class="p">},</span>
</pre></div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Multiple AJAX request</h3>
<div class="outline-text-3" id="text-2-3">
<div class="highlight"><pre><span></span><span class="kd">function</span> <span class="nx">getCookie</span><span class="p">(</span><span class="nx">name</span><span class="p">)</span> <span class="p">{</span>
    <span class="kd">var</span> <span class="nx">cookieValue</span> <span class="o">=</span> <span class="kc">null</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">document</span><span class="p">.</span><span class="nx">cookie</span> <span class="o">&amp;&amp;</span> <span class="nb">document</span><span class="p">.</span><span class="nx">cookie</span> <span class="o">!==</span> <span class="s1">''</span><span class="p">)</span> <span class="p">{</span>
	<span class="kd">var</span> <span class="nx">cookies</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">cookie</span><span class="p">.</span><span class="nx">split</span><span class="p">(</span><span class="s1">';'</span><span class="p">);</span>
	<span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">cookies</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
	    <span class="kd">var</span> <span class="nx">cookie</span> <span class="o">=</span> <span class="nx">jQuery</span><span class="p">.</span><span class="nx">trim</span><span class="p">(</span><span class="nx">cookies</span><span class="p">[</span><span class="nx">i</span><span class="p">]);</span>
	    <span class="c1">// Does this cookie string begin with the name we want?</span>
	    <span class="k">if</span> <span class="p">(</span><span class="nx">cookie</span><span class="p">.</span><span class="nx">substring</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nx">name</span><span class="p">.</span><span class="nx">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">===</span> <span class="p">(</span><span class="nx">name</span> <span class="o">+</span> <span class="s1">'='</span><span class="p">))</span> <span class="p">{</span>
		<span class="nx">cookieValue</span> <span class="o">=</span> <span class="nb">decodeURIComponent</span><span class="p">(</span><span class="nx">cookie</span><span class="p">.</span><span class="nx">substring</span><span class="p">(</span><span class="nx">name</span><span class="p">.</span><span class="nx">length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">));</span>
		<span class="k">break</span><span class="p">;</span>
	    <span class="p">}</span>
	<span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="nx">cookieValue</span><span class="p">;</span>
<span class="p">}</span>
<span class="kd">var</span> <span class="nx">csrftoken</span> <span class="o">=</span> <span class="nx">getCookie</span><span class="p">(</span><span class="s1">'csrftoken'</span><span class="p">);</span>

<span class="kd">function</span> <span class="nx">csrfSafeMethod</span><span class="p">(</span><span class="nx">method</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// these HTTP methods do not require CSRF protection</span>
    <span class="k">return</span> <span class="p">(</span><span class="sr">/^(GET|HEAD|OPTIONS|TRACE)$/</span><span class="p">.</span><span class="nx">test</span><span class="p">(</span><span class="nx">method</span><span class="p">));</span>
<span class="p">}</span>
<span class="nx">$</span><span class="p">.</span><span class="nx">ajaxSetup</span><span class="p">({</span>
    <span class="nx">beforeSend</span><span class="o">:</span> <span class="kd">function</span><span class="p">(</span><span class="nx">xhr</span><span class="p">,</span> <span class="nx">settings</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">csrfSafeMethod</span><span class="p">(</span><span class="nx">settings</span><span class="p">.</span><span class="nx">type</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="k">this</span><span class="p">.</span><span class="nx">crossDomain</span><span class="p">)</span> <span class="p">{</span>
	    <span class="nx">xhr</span><span class="p">.</span><span class="nx">setRequestHeader</span><span class="p">(</span><span class="s2">"X-CSRFToken"</span><span class="p">,</span> <span class="nx">csrftoken</span><span class="p">);</span>
	<span class="p">}</span>
    <span class="p">}</span>
<span class="p">});</span>
</pre></div>


<p>
Ref:
</p>
<ol class="org-ol">
<li>
<a href="https://docs.djangoproject.com/en/2.1/ref/csrf/">Cross Site Request Forgery protection</a>
</li>
<li>
<a href="https://github.com/django/django/blob/master/django/middleware/csrf.py">csrf.py</a>
</li>
<li>
<a href="https://stackoverflow.com/questions/48002861/whats-the-relationship-between-csrfmiddlewaretoken-and-csrftoken">What's the relationship between csrfmiddlewaretoken and csrftoken?</a> 
</li>
</ol>
</div>
</div>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/create-node-benchmark-in-py2neo/" class="u-url">Create Node Benchmark in Py2neo</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-11-05T22:30:05+08:00" title="2018-11-05 22:30">2018-11-05 22:30</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>
Recently, I'm working on a neo4j project. I use <code>=Py2neo=</code> to interact with graph db. Alghough <code>=Py2neo=</code> is a very pythonic and easy to use, its performance is really poor. Sometimes I have to manually write cypher statement by myself if I can't bear with the slow excution. Here is a small script which I use to compare the performance of 4 diffrent ways to insert nodes.
</p>

<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="kn">from</span> <span class="nn">graph_db</span> <span class="kn">import</span> <span class="n">graph</span>

<span class="kn">from</span> <span class="nn">py2neo.data</span> <span class="kn">import</span> <span class="n">Node</span><span class="p">,</span> <span class="n">Subgraph</span>


<span class="k">def</span> <span class="nf">delete_label</span><span class="p">(</span><span class="n">label</span><span class="p">):</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s1">'MATCH (n:{}) DETACH DELETE n'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">delete_all</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'delete all'</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s1">'match (n) detach delete n'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">count_label</span><span class="p">(</span><span class="n">label</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">nodes</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">bench_create1</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Using py2neo one by one'</span><span class="p">)</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
	<span class="n">n</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="s1">'test'</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
	<span class="n">tx</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">count_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">))</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bench_create2</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Using cypher one by one'</span><span class="p">)</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
	<span class="n">tx</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s1">'create (n:test {id: $id})'</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
	<span class="k">if</span> <span class="n">i</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
	    <span class="n">tx</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
	    <span class="n">tx</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">count_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">))</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bench_create3</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Using Subgraph'</span><span class="p">)</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
	<span class="n">nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Node</span><span class="p">(</span><span class="s1">'test'</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">Subgraph</span><span class="p">(</span><span class="n">nodes</span><span class="o">=</span><span class="n">nodes</span><span class="p">)</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">count_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">))</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bench_create4</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Using unwind'</span><span class="p">)</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">))</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s1">'unwind $ids as id create (n:test {id: id})'</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="n">ids</span><span class="p">)</span>
    <span class="n">tx</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">count_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">))</span>
    <span class="n">delete_label</span><span class="p">(</span><span class="s1">'test'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bench_create</span><span class="p">():</span>
    <span class="n">create_tests</span> <span class="o">=</span> <span class="p">[</span><span class="n">bench_create1</span><span class="p">,</span> <span class="n">bench_create2</span><span class="p">,</span> <span class="n">bench_create3</span><span class="p">,</span> <span class="n">bench_create4</span><span class="p">]</span>

    <span class="k">print</span><span class="p">(</span><span class="s1">'testing create'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">create_tests</span><span class="p">:</span>
	<span class="n">i</span><span class="p">()</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">bench_create</span><span class="p">()</span>
</pre></div>

<p>
Apparently, using cypher with <code>=unwind=</code> keyword is the fastest way to batch insert nodes.
</p>
<div class="highlight"><pre><span></span>testing create
Using py2neo one by one
96.09799289703369
100000
Using cypher one by one
9.493892192840576
100000
Using Subgraph
7.638832092285156
100000
Using unwind
2.511630058288574
100000
</pre></div>

<p>
The above result is baed on <code>=http=</code> protocal. An very interesting result is that, <code>=bolt=</code> protocal will decrease the time of the first method, but double the time of sencond method. That's wired, maybe <code>=py2neo=</code> has some special opitimusation when doing batch insert on <code>=bolt=</code> protocal? But I have no idea why insert one by one with cypher is 2x slower. Here is the result of <code>=bolt=</code> protocal.
</p>
<div class="highlight"><pre><span></span>testing create
Using py2neo one by one
51.73185706138611
100000
Using cypher one by one
22.051995992660522
100000
Using Subgraph
8.81674599647522
100000
Using unwind
2.8623900413513184
100000
</pre></div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/deploy-nikola-orgmode-on-travis/" class="u-url">Deploy Nikola OrgMode on Travis</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-11-03T20:20:45+08:00" title="2018-11-03 20:20">2018-11-03 20:20</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>
Recently, I enjoy using <code>=Spacemacs=</code>, so I decided to switch to org file from Markdown for writing blog. After several attempts, I managed to let Travis convert org file to HTML. Here are the steps.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Install orgmode plugin</h2>
<div class="outline-text-2" id="text-1">
<p>
First you need to install orgmode plugin on your computer following the official guide: <a href="https://plugins.getnikola.com/v8/orgmode/">Nikola orgmode plugin</a>.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Edit <code>=conf.el=</code>
</h2>
<div class="outline-text-2" id="text-2">
<p>
<code>=OrgMode=</code> will convert to HTML to display on Nikola. Orgmode plugin will call Emacs to do this job. When I run <code>=nikola build=</code>, it shows this message: <code>=Please install htmlize from https://github.com/hniksic/emacs-htmlize=</code>. I'm using <code>=Spacemacs=</code>, the <code>=htmlize=</code> package is already downloaded if the <code>=org=</code> layer is enabled. I just need to add htmlize folder to load-path. So here is the code:
</p>
<div class="highlight"><pre><span></span>(setq dir "~/.emacs.d/elpa/27.0/develop/")
(if(file-directory-p dir)
    (let ((default-directory dir))
      (normal-top-level-add-subdirs-to-load-path)))
(require 'htmlize)
</pre></div>

<p>
This package is also needed on Travis, the similar approach is required.
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Modify <code>=.travis.yml=</code>
</h2>
<div class="outline-text-2" id="text-3">
<p>
Travis is using ubuntu 14.04, and the default Emacs version is 24, and the orgmode version is below 8.0, which not match the requirements. The easiest solution is to update Emacs to 25. So in the <code>=before_install=</code> section, add these code:
</p>
<div class="highlight"><pre><span></span>- sudo add-apt-repository ppa:kelleyk/emacs -y
- sudo apt-get update
</pre></div>
<p>
In the <code>=install=</code> section, add these code:
</p>
<div class="highlight"><pre><span></span>- sudo apt-get remove emacs
- sudo apt autoremove
- sudo apt-get install emacs25
</pre></div>

<p>
The default emacs doesn't contains <code>=htmlize=</code> package. So add <code>=git clone https://github.com/hniksic/emacs-htmlize ~/emacs-htmlize=</code> into <code>=before_install=</code> section.
</p>

<p>
Finally, modify <code>=conf.el=</code> for Travis Emacs, add GitHub repo to <code>=load-path=</code>: <code>=(add-to-list 'load-path "~/emacs-htmlize/")=</code>
</p>

<p>
Voila, the org file should show up.
</p>

<p>
The full <code>=.travis.yml=</code> is below:
</p>
<div class="highlight"><pre><span></span>language: python
cache: apt
sudo: false
addons:
  apt:
    packages:
    - language-pack-en-base
branches:
  only:
  - src
python:
- 3.6
before_install:
- sudo add-apt-repository ppa:kelleyk/emacs -y
- sudo apt-get update
- openssl aes-256-cbc -K $encrypted_a5c638e4bedc_key -iv $encrypted_a5c638e4bedc_iv
  -in travis.enc -out travis -d
- git config --global user.name 'bebound'
- git config --global user.email 'bebound@gmail.com'
- git config --global push.default 'simple'
- pip install --upgrade pip wheel
- echo -e 'Host github.com\n    StrictHostKeyChecking no' &gt;&gt; ~/.ssh/config
- eval "$(ssh-agent -s)"
- chmod 600 travis
- ssh-add travis
- git remote rm origin
- git remote add origin git@github.com:bebound/bebound.github.io
- git fetch origin master
- git branch master FETCH_HEAD
- git clone https://github.com/hniksic/emacs-htmlize ~/emacs-htmlize
install:
- pip install 'Nikola[extras]'==7.8.15
- sudo apt-get remove emacs
- sudo apt autoremove
- sudo apt-get install emacs25
script:
- nikola build &amp;&amp; nikola github_deploy -m 'Nikola auto deploy [ci skip]'
notifications:
  email:
    on_success: change
    on_failure: always
</pre></div>

<p>
And here is the <code>=conf.el=</code>:
</p>
<div class="highlight"><pre><span></span>(setq dir "~/.emacs.d/elpa/27.0/develop/")
(if(file-directory-p dir)
    (let ((default-directory dir))
      (normal-top-level-add-subdirs-to-load-path)))
(add-to-list 'load-path "~/emacs-htmlize/")
(require 'htmlize)
</pre></div>
</div>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/using-chinese-characters-in-matplotlib/" class="u-url">Using Chinese characters in Matplotlib</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-10-04T21:47:37+08:00" title="2018-10-04 21:47">2018-10-04 21:47</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>After searching from Google, here is easiest solution. This should also works on other languages:</p>
<pre class="code literal-block"><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">'retina'</span>

<span class="kn">import</span> <span class="nn">matplotlib.font_manager</span> <span class="kn">as</span> <span class="nn">fm</span>
<span class="n">f</span> <span class="o">=</span> <span class="s2">"/System/Library/Fonts/PingFang.ttc"</span>
<span class="n">prop</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"ä½ å¥½"</span><span class="p">,</span><span class="n">fontproperties</span><span class="o">=</span><span class="n">prop</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>


<p>Output:
<img alt="LSTM" src="images/matplot_chinese.png"></p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/lstm-and-gru/" class="u-url">LSTM and GRU</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-04-22T23:27:45+08:00" title="2018-04-22 23:27">2018-04-22 23:27</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>LSTM</h3>
<p>The avoid the problem of vanishing gradient and exploding gradient in vanilla RNN, LSTM was published, which can remember information for longer periods of time.</p>
<p>Here is the structure of LSTM:</p>
<p><img alt="LSTM" src="images/LSTM_LSTM.png"></p>
<p>The calculate procedure are:</p>
<div>

$$
\begin{aligned}
f_t&amp;=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\
i_t&amp;=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\
o_t&amp;=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\
\tilde{C_t}&amp;=tanh(W_C\cdot[h_{t-1},x_t]+b_C)\\
C_t&amp;=f_t\ast C_{t-1}+i_t\ast \tilde{C_t}\\
h_t&amp;=o_t \ast tanh(C_t)
\end{aligned}
$$

</div>

<p>$f_t$,$i_t$,$o_t$ are forget gate, input gate and output gate respectively. $\tilde{C_t}$ is the new memory content. $C_t$ is cell state. $h_t$ is the output. </p>
<p>Use $f_t$ and $i_t$ to update $C_t$, use $o_t$ to decide which part of hidden state should be outputted.</p>
<h3>GRU</h3>
<p><img alt="LSTM" src="images/LSTM_GRU.png"></p>
<div>

$$
\begin{aligned}
z_t&amp;=\sigma(W_z\cdot[h_{t-1},x_t])\\
r_t&amp;=\sigma(W_r\cdot[h_{t-1},x_t])\\
\tilde{h_t}&amp;=tanh(W\cdot[r_t \ast h_{t-1},x_t])\\
h_t&amp;=(1-z_t)\ast h_{t-1}+z_t \ast \tilde{h_t}
\end{aligned}
$$

</div>

<p>$z_t$ is update gate, $r_t$ is reset gate, $\tilde{h_t}$ is candidate activation, $h_t$ is activation.</p>
<p>Compare with LSTM, GRU merge cell state and hidden state to one hiddent state, and use $z_t$ to decide how to update the state rather than $f_t$ and $i_t$.</p>
<p>Update:</p>
<p>Here is a good article: <a href="https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Illustrated Guide to LSTMâs and GRUâs: A step by step explanation</a></p>
<p>Refï¼</p>
<ul>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a href="https://blog.csdn.net/zhangxb35/article/details/70060295">RNN, LSTM, GRU å¬å¼æ»ç»</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/models-and-architechtures-in-word2vec/" class="u-url">Models and Architechtures in Word2vec</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2018-01-05T21:57:17+08:00" title="2018-01-05 21:57">2018-01-05 21:57</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Models</h3>
<h4>CBOW (Continuous Bag of Words)</h4>
<p>Use the context to predict the probability of current word.</p>
<p><img alt="cbow" src="images/doc2vec_cbow.png"></p>
<ol>
<li>Context words' vectors are $\upsilon_{c-n} ... \upsilon_{c+m}$ ($m$ is the window size)</li>
<li>Context vector $ \hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+...+\upsilon_{c+m}}{2m} $</li>
<li>Score vector $z_i = u_i\hat{\upsilon}$, where $u_i$ is the output vector representation of word $\omega_i$</li>
<li>Turn scores into probabilities $\hat{y}=softmax(z)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<p>We use cross entropy $H(\hat{y},y)$ to measure the distance between these two distributions.
$$H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}$$</p>
<p>$y$ and $\hat{y}$ is accurate, so the loss simplifies to:
$$H(\hat{y},y)=-y_j\log(\hat{y})$$</p>
<p>For perfect prediction, $H(\hat{y},y)=-1\log(1)=0$</p>
<p>According to this, we can create this loss function:</p>
<div>

$$\begin{aligned}
minimize\ J &amp;=-\log P(\omega_c\lvert \omega_{c-m},...,\omega_{c-1},...,\omega_{c+m}) \\
&amp;= -\log P(u_c \lvert \hat{\upsilon}) \\
&amp;= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\
&amp;= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}$$

</div>

<h4>Skip-Gram</h4>
<p>Use current word to predict its context.</p>
<p><img alt="cbow" src="images/doc2vec_skip-gram.png"></p>
<ol>
<li>We get the input word's vector $\upsilon_c$</li>
<li>Generate $2m$ score vectors, $uc_{c-m},...,u_{c-1},...,u_{c+m}$.</li>
<li>Turn scores into probabilities $\hat{y}=softmax(u)$</li>
<li>We desire probabilities $\hat{y}$ match the true probabilities $y$.</li>
</ol>
<div>

$$\begin{aligned}
minimize J &amp;=-\log P(\omega_{c-m},...,\omega_{c-1},\omega_{c+1},...\omega_{c+m}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\
&amp;=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\
&amp;=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}$$

</div>

<h3>Models</h3>
<p>Minimize $J$ is expensive, as the summation is over $\lvert V \rvert$. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.</p>
<h4>Hierarchical Softmax</h4>
<p>Encode words into a huffman tree, then each word has a Huffman code. The probability of it's probability $P(w\lvert Context(\omega))$ can change to choose the right path from root the che leaf node, each node is a binary classification. Suppose code $0$ is a possitive label, $1$ is negative label. If the probability of a possitive classification is 
$$
\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}
$$</p>
<p>Then the probability of negative classification is
$$
1-\sigma(X^T_\omega \theta)
$$</p>
<p><img alt="cbow" src="images/doc2vec_hierarchical_softmax.png">
è¶³ç's Huffman code is $1001$, then it's probability in each node are</p>
<div>

$$\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&amp;=1-\sigma(X^T_\omega \theta^\omega_1))\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&amp;=\sigma(X^T_\omega \theta^\omega_2))\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&amp;=\sigma(X^T_\omega \theta^\omega_3))\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&amp;=1-\sigma(X^T_\omega \theta^\omega_4))\\
\end{aligned}$$

</div>

<p>where $\theta$ is prarameter in the node.</p>
<p>The probability of the <code>è¶³ç</code> is the production of these equation.</p>
<p>Generally,</p>
<div>

$$p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})$$

</div>

<h4>Negative Sampling</h4>
<p>Choose some negitive sample, add the probability of the negative word into loss function. Maximize the positive words' probability and minimize the negitive words' probability.</p>
<p>Let $P(D=0 \lvert \omega,c)$ be the probability that $(\omega,c)$ did not come from the corpus data. Then the objective funtion will be</p>
<div>

$$\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)$$

</div>

<p>where $\theta$ is the parameters of the model($\upsilon$ and $u$).</p>
<p>Ref:</p>
<ul>
<li><a href="http://www.hankcs.com/nlp/word2vec.html">word2vecåçæ¨å¯¼ä¸ä»£ç åæ</a></li>
<li><a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf">CS 224D: Deep Learning for NLP Lecture Notes: Part I</a></li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969519">word2vec ä¸­çæ°å­¦åçè¯¦è§£ï¼ä¸ï¼ç®å½ååè¨</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/" class="u-url">Semi-supervised text classification using doc2vec and label spreading</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-09-10T22:18:15+08:00" title="2017-09-10 22:18">2017-09-10 22:18</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Here is a simple way to classify text without much human effort and get a impressive performance.</p>
<p>It can be divided into two steps:</p>
<ol>
<li>Get train data by using keyword classificaton</li>
<li>Generate a more accurate classification model by using doc2vec and label spreading</li>
</ol>
<h4>Keyword-based Classification</h4>
<p>Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candicate.</p>
<ol>
<li>Find some most common words to classify the text.</li>
<li>Use this equition to calculate the score of each word appears in the text.
   $$ score(i) = \frac{count(i)}{all\_count(i)^{0.3}}$$
   which $all\_count(i)$ is the word i's wordc ount in all corpus, and $count(i)$ is the word i's word count in positive corpus.</li>
<li>Check the top words, add it to the final keyword list. Repeat this process.</li>
</ol>
<p>Finally, we can use the keywords to classify the text and get the train data. </p>
<h4>Classification by <code>doc2vec</code> and Label Spreading</h4>
<p>Keyword-based classification sometimes produces the wrong result, as it can't using the symantic information in the text. Fortunately, Google has open sourced <code>word2vec</code>, which can be used to produce semantically meaningful word embeddings. Furthermore, sentences can also be converted to vectors by using <code>doc2vec</code>. Sentences which has closed meaning also have short vector distance.</p>
<p>So the problem is how to classify these vectors.</p>
<ol>
<li>Using corpus to train the <code>doc2vec</code> model.</li>
<li>Using <code>doc2vec</code> model to convert sentence into vector.</li>
<li>Using label spreading algorithm to train a classify model to classify the vectors.</li>
</ol>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/parameters-in-dov2vec/" class="u-url">Parameters in dov2vec</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-08-03T00:02:17+08:00" title="2017-08-03 00:02">2017-08-03 00:02</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Here are some parameter in <code>gensim</code>'s <code>doc2vec</code> class.</p>
<h4>window</h4>
<p>window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.</p>
<p>In <code>skip-gram</code> model, if the window size is 2, the training samples will be this:(the blue word is the input word)</p>
<p><img alt="window" src="images/doc2vec_window.png"></p>
<h4>min_count</h4>
<p>If the word appears less than this value, it will be skipped</p>
<h4>sample</h4>
<p>High frequency word like <code>the</code> is useless for training. <code>sample</code> is a threshold for deleting these higher-frequency words. The probability of keeping the word $w_i$ is:</p>
<p>$$P(w_i) = (\sqrt{\frac{z(\omega_i)}{s}} + 1) \cdot \frac{s}{z(\omega_i)}$$</p>
<p>where $z(w_i)$ is the frequency of the word and $s$ is the sample rate.</p>
<p>This is the plot when <code>sample</code> is 1e-3.</p>
<p><img alt="negative-sample" src="images/doc2vec_negative_sample.png"></p>
<h4>negative</h4>
<p>Usually, when training a neural network, for each training sample, all of the weights in the neural network need to be tweaked. For example, if the word pair is ('fox', 'quick'), then only the word quick's neurons should output 1, and all of the other word neurons should output 0.</p>
<p>But it would takes a lot of time to do this when we have billions of training samples. So, instead of update all of the weight, we random choose a small number of "negative" words (default value is 5) to update the weight.(Update their wight to output 0).</p>
<p>So when dealing with word pair ('fox','quick'), we update quick's weight to output 1, and other 5 random words' wight to output 1.</p>
<p>The probability of selecting word $\omega_i$ is $P(\omega_i)$:</p>
<p>$$P(\omega_i) = \frac{  {f(\omega_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(\omega_j)}^{3/4} \right) }$$</p>
<p>$f(\omega_j)$ is the frequency of word $\omega_j$.</p>
<p>Ref:</p>
<ul>
<li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></li>
<li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">Word2Vec Tutorial Part 2 - Negative Sampling</a></li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/brief-introduction-of-label-propagation-algorithm/" class="u-url">Brief Introduction of Label Propagation Algorithm</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-07-16T23:46:04+08:00" title="2017-07-16 23:46">2017-07-16 23:46</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>As I said before, I'm working on a text classification project. I use <code>doc2vec</code> to convert text into vectors, then I use LPA to classify the vectors.</p>
<p>LPA is a simple, effective semi-supervised algotithm. It can use the density of unlabeled data to find a hyperplane to split the data.</p>
<p>Here are the main steps of the algorithm:</p>
<ol>
<li>Let $ (x_1,y1)...(x_l,y_l)$ be labeled data, $Y_L = {y_1...y_l} $ are the class labels. Let $(x_{l+1},y_{l+u})$ be unlabeled data where $Y_U = {y_{l+1}...y_{l+u}}$ are unobserved, useally $l \ll u$. Let $X={x_1...x_{l+u}}$ where $x_i\in R^D$. The problem is to estimate $Y_U$ for $X$ and $Y_L$.</li>
<li>Calculate the similarity of the data points. The most simple metric is Euclidean distance. Use a parameter $\sigma$ to cotrol the weights.</li>
</ol>
<p>$$w_{ij}= exp(-\frac{d^2_{ij}}{\sigma^2})=exp(-\frac{\sum^D_{d=1}{(x^d_i-x^d_j})^2}{\sigma^2})$$</p>
<p>Larger weight allow labels to travel through easier.</p>
<ol>
<li>Define a $(l+u)*(l+u)$ probabilistic transition matrix $T$</li>
</ol>
<div>

$$T_{ij}=P(j \rightarrow i)=\frac{w_{ij}}{\sum^{l+u}_{k=1}w_{kj}}$$

</div>

<p>$T_{ij}$ is the probability to jump from node $j$ to $i$. If there are $C$ classes, we can define a $(l+u)*C$ label matrix $Y$, to represent the probability of a label belong to class $c$. The initialiation of unlabeled data points is not important.</p>
<ol>
<li>Propagate $Y \leftarrow TY$</li>
<li>Row-normalize Y.</li>
<li>Reset labeled data's Y. Repeat 3 until Y converges.</li>
</ol>
<p>In short, let the nearest label has larger weight, then calculate each label's new label, reset labeled data's label, repeat.</p>
<p><img alt="label spreading" src="images/label_spreading.png"></p>
<p>Ref:</p>
<ol>
<li>
<p><a href="http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf">Learning from Labeled and Unlabeled Data with Label Propagation</a></p>
</li>
<li>
<p><a href="http://blog.csdn.net/zouxy09/article/details/49105265">æ ç­¾ä¼ æ­ç®æ³ï¼Label Propagationï¼åPythonå®ç°</a></p>
</li>
</ol>
</div>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="kkblog-1";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script>
</div>

    
    
        

</body>
</html>
