<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>KK's Blog (Posts about mathjax)</title><link>https://bebound.github.io/</link><description></description><atom:link href="https://bebound.github.io/categories/mathjax.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2017 &lt;a href="mailto:bebound gm@il.com"&gt;Kurt Lei&lt;/a&gt; </copyright><lastBuildDate>Sun, 10 Sep 2017 14:37:57 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Semi-supervised text classification using doc2vec and label spreading</title><link>https://bebound.github.io/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</link><dc:creator>Kurt Lei</dc:creator><description>&lt;div&gt;&lt;p&gt;Here is a simple way to classify text without much human effort and get a impressive performance.&lt;/p&gt;
&lt;p&gt;It can be divided into two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get train data by using keyword classificaton&lt;/li&gt;
&lt;li&gt;Generate a more accurate classification model by using doc2vec and label spreading&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Keyword-based Classification&lt;/h4&gt;
&lt;p&gt;Keyword based classification is a simple but effective method. Extracting the target keyword is a monotonous work. I use this method to automatic extract keyword candicate.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find some most common words to classify the text.&lt;/li&gt;
&lt;li&gt;Use this equition to calculate the score of each word appears in the text.
   $$ score(i) = \frac{count(i)}{all_count(i)^{0.3}}$$
   which $all_count(i)$ is the word i's wordc ount in all corpus, and $count(i)$ is the word i's word count in positive corpus.&lt;/li&gt;
&lt;li&gt;Check the top words, add it to the final keyword list. Repeat this process.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, we can use the keywords to classify the text and get the train data. &lt;/p&gt;
&lt;h4&gt;Classification by &lt;code&gt;doc2vec&lt;/code&gt; and Label Spreading&lt;/h4&gt;
&lt;p&gt;Keyword-based classification sometimes produces the wrong result, as it can't using the symantic information in the text. Fortunately, Google has open sourced &lt;code&gt;word2vec&lt;/code&gt;, which can be used to produce word embedding. Furthermore, sentences can also be converted to vectors by using &lt;code&gt;doc2vec&lt;/code&gt;. Sentences which has closed meaning also have small vector distance.&lt;/p&gt;
&lt;p&gt;So the problem is how to classify these vectors.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Using corpus to train the &lt;code&gt;doc2vec&lt;/code&gt; model.&lt;/li&gt;
&lt;li&gt;Using &lt;code&gt;doc2vec&lt;/code&gt; model to convert sentence into vector.&lt;/li&gt;
&lt;li&gt;Using label spreading algorithm to train a classify model to classify the vectors.&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>mathjax</category><guid>https://bebound.github.io/posts/semi-supervised-text-classification-using-doc2vec-and-label-spreading/</guid><pubDate>Sun, 10 Sep 2017 14:18:15 GMT</pubDate></item><item><title>Parameters in dov2vec</title><link>https://bebound.github.io/posts/parameters-in-dov2vec/</link><dc:creator>Kurt Lei</dc:creator><description>&lt;div&gt;&lt;p&gt;Here are some parameter in &lt;code&gt;gensim&lt;/code&gt;'s &lt;code&gt;doc2vec&lt;/code&gt; class.&lt;/p&gt;
&lt;h4&gt;window&lt;/h4&gt;
&lt;p&gt;window is the maximum distance between the predicted word and context words used for prediction within a document. It will look behind and ahead.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;skip-gram&lt;/code&gt; model, if the window size is 2, the training samples will be this:(the blue word is the input word)&lt;/p&gt;
&lt;p&gt;&lt;img alt="window" src="https://bebound.github.io/images/doc2vec_window.png"&gt;&lt;/p&gt;
&lt;h4&gt;min_count&lt;/h4&gt;
&lt;p&gt;If the word appears less than this value, it will be skipped&lt;/p&gt;
&lt;h4&gt;sample&lt;/h4&gt;
&lt;p&gt;High frequency word like &lt;code&gt;the&lt;/code&gt; is useless for training. &lt;code&gt;sample&lt;/code&gt; is a threshold for deleting these higher-frequency words. The probability of keeping the word $w_i$ is:&lt;/p&gt;
&lt;p&gt;$$P(w_i) = (\sqrt{\frac{z(w_i)}{s}} + 1) \cdot \frac{s}{z(w_i)}$$&lt;/p&gt;
&lt;p&gt;where $z(w_i)$ is the frequency of the word and $s$ is the sample rate.&lt;/p&gt;
&lt;p&gt;This is the plot when &lt;code&gt;sample&lt;/code&gt; is 1e-3.&lt;/p&gt;
&lt;p&gt;&lt;img alt="sample" src="https://bebound.github.io/images/doc2vec_sample.png"&gt;&lt;/p&gt;
&lt;h4&gt;negative&lt;/h4&gt;
&lt;p&gt;Usually, when training a neural network, for each training sample, all of the weights in the neural network need to be tweaked. For example, if the word pair is ('fox', 'quick'), then only the word quick's neurons should output 1, and all of the other word neurons should output 0.&lt;/p&gt;
&lt;p&gt;But it would takes a lot of time to do this when we have billions of training samples. So, instead of update all of the weight, we random choose a small number of "negative" words (default value is 5) to update the weight.(Update their wight to output 0).&lt;/p&gt;
&lt;p&gt;So when dealing with word pair ('fox','quick'), we update quick's weight to output 1, and other 5 random words' wight to output 1.&lt;/p&gt;
&lt;p&gt;The probability of selecting word $w_i$ is $P(w_i)$:&lt;/p&gt;
&lt;p&gt;$$P(w_i) = \frac{  {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left(  {f(w_j)}^{3/4} \right) }$$&lt;/p&gt;
&lt;p&gt;$f(w_j)$ is the frequency of word $w_j$.&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"&gt;Word2Vec Tutorial - The Skip-Gram Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"&gt;Word2Vec Tutorial Part 2 - Negative Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>mathjax</category><guid>https://bebound.github.io/posts/parameters-in-dov2vec/</guid><pubDate>Wed, 02 Aug 2017 16:02:17 GMT</pubDate></item><item><title>Brief Introduction of Label Propagation Algorithm</title><link>https://bebound.github.io/posts/brief-introduction-of-label-propagation-algorithm/</link><dc:creator>Kurt Lei</dc:creator><description>&lt;div&gt;&lt;p&gt;As I said before, I'm working on a text classification project. I use &lt;code&gt;doc2vec&lt;/code&gt; to convert text into vectors, then I use LPA to classify the vectors.&lt;/p&gt;
&lt;p&gt;LPA is a simple, effective semi-supervised algotithm. It can use the density of unlabeled data to find a hyperplane to split the data.&lt;/p&gt;
&lt;p&gt;Here are the main stop of the algorithm:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $ (x_1,y1)...(x_l,y_l)$ be labeled data, $Y_L = {y_1...y_l} $ are the class labels. Let $(x_{l+1},y_{l+u})$ be unlabeled data where $Y_U = {y_{l+1}...y_{l+u}}$ are unobserved, useally $l \ll u$. Let $X={x_1...x_{l+u}}$ where $x_i\in R^D$. The problem is to estimate $Y_U$ for $X$ and $Y_L$.&lt;/li&gt;
&lt;li&gt;Calculate the similarity of the data points. The most simple metric is Euclidean distance. Use a parameter $\sigma$ to cotrol the weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$w_{ij}= exp(-\frac{d^2_{ij}}{\sigma^2})=exp(-\frac{\sum^D_{d=1}{(x^d_i-x^d_j})^2}{\sigma^2})$$&lt;/p&gt;
&lt;p&gt;Larger weight allow labels to travel through easier.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define a $(l+u)*(l+u)$ probabilistic transition matrix $T$&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;

$$T_{ij}=P(j \rightarrow i)=\frac{w_{ij}}{\sum^{l+u}_{k=1}w_{kj}}$$

&lt;/div&gt;

&lt;p&gt;$T_{ij}$ is the probability to jump from node $j$ to $i$. If there are $C$ classes, we can define a $(l+u)*C$ label matrix $Y$, to represent the probability of a label belong to class $c$. The initialiation of unlabeled data points is not important.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Propagate $Y \leftarrow TY$&lt;/li&gt;
&lt;li&gt;Row-normalize Y.&lt;/li&gt;
&lt;li&gt;Reset labeled data's Y. Repeat 3 until Y converges.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In short, let the nearest label has larger weight, then calculate each label's new label, reset labeled data's label, repeat.&lt;/p&gt;
&lt;p&gt;&lt;img alt="label spreading" src="https://bebound.github.io/images/label_spreading.png"&gt;&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf"&gt;Learning from Labeled and Unlabeled Data with Label Propagation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/49105265"&gt;标签传播算法（Label Propagation）及Python实现&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>mathjax</category><guid>https://bebound.github.io/posts/brief-introduction-of-label-propagation-algorithm/</guid><pubDate>Sun, 16 Jul 2017 15:46:04 GMT</pubDate></item></channel></rss>