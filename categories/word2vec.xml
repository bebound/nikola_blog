<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>KK's Blog (Posts about word2vec)</title><link>https://blog.fromkk.com/</link><description></description><atom:link href="https://blog.fromkk.com/categories/word2vec.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:bebound gm@il.com"&gt;Kurt Lei&lt;/a&gt; </copyright><lastBuildDate>Sun, 02 Dec 2018 08:09:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Models and Architechtures in Word2vec</title><link>https://blog.fromkk.com/posts/models-and-architechtures-in-word2vec/</link><dc:creator>Kurt Lei</dc:creator><description>&lt;div&gt;&lt;h3&gt;Models&lt;/h3&gt;
&lt;h4&gt;CBOW (Continuous Bag of Words)&lt;/h4&gt;
&lt;p&gt;Use the context to predict the probability of current word.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cbow" src="https://blog.fromkk.com/images/doc2vec_cbow.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Context words' vectors are $\upsilon_{c-n} ... \upsilon_{c+m}$ ($m$ is the window size)&lt;/li&gt;
&lt;li&gt;Context vector $ \hat{\upsilon}=\frac{\upsilon_{c-m}+\upsilon_{c-m+1}+...+\upsilon_{c+m}}{2m} $&lt;/li&gt;
&lt;li&gt;Score vector $z_i = u_i\hat{\upsilon}$, where $u_i$ is the output vector representation of word $\omega_i$&lt;/li&gt;
&lt;li&gt;Turn scores into probabilities $\hat{y}=softmax(z)$&lt;/li&gt;
&lt;li&gt;We desire probabilities $\hat{y}$ match the true probabilities $y$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We use cross entropy $H(\hat{y},y)$ to measure the distance between these two distributions.
$$H(\hat{y},y)=-\sum_{j=1}^{\lvert V \rvert}{y_j\log(\hat{y}_j)}$$&lt;/p&gt;
&lt;p&gt;$y$ and $\hat{y}$ is accurate, so the loss simplifies to:
$$H(\hat{y},y)=-y_j\log(\hat{y})$$&lt;/p&gt;
&lt;p&gt;For perfect prediction, $H(\hat{y},y)=-1\log(1)=0$&lt;/p&gt;
&lt;p&gt;According to this, we can create this loss function:&lt;/p&gt;
&lt;div&gt;

$$\begin{aligned}
minimize\ J &amp;amp;=-\log P(\omega_c\lvert \omega_{c-m},...,\omega_{c-1},...,\omega_{c+m}) \\
&amp;amp;= -\log P(u_c \lvert \hat{\upsilon}) \\
&amp;amp;= -\log \frac{\exp(u_c^T\hat{\upsilon})}{\sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})} \\
&amp;amp;= -u_c^T\hat{\upsilon}+\log \sum_{j=1}^{\lvert V \rvert}\exp (u_j^T\hat{\upsilon})
\end{aligned}$$

&lt;/div&gt;

&lt;h4&gt;Skip-Gram&lt;/h4&gt;
&lt;p&gt;Use current word to predict its context.&lt;/p&gt;
&lt;p&gt;&lt;img alt="cbow" src="https://blog.fromkk.com/images/doc2vec_skip-gram.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We get the input word's vector $\upsilon_c$&lt;/li&gt;
&lt;li&gt;Generate $2m$ score vectors, $uc_{c-m},...,u_{c-1},...,u_{c+m}$.&lt;/li&gt;
&lt;li&gt;Turn scores into probabilities $\hat{y}=softmax(u)$&lt;/li&gt;
&lt;li&gt;We desire probabilities $\hat{y}$ match the true probabilities $y$.&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;

$$\begin{aligned}
minimize J &amp;amp;=-\log P(\omega_{c-m},...,\omega_{c-1},\omega_{c+1},...\omega_{c+m}\lvert \omega_c)\\
&amp;amp;=-\log \prod_{j=0,j\ne m}^{2m}P(\omega_{c-m+j}\lvert \omega_c)\\
&amp;amp;=-\log \prod_{j=0,j\ne m}^{2m}P(u_{c-m+j}\lvert \upsilon_c)\\
&amp;amp;=-\log \prod_{j=0,j\ne m}^{2m}\frac{\exp (u^T_{c-m+j}\upsilon_c)}{\sum_{k=1}^{\lvert V \rvert}{\exp (u^T_k \upsilon_c)}}\\
&amp;amp;=-\sum_{j=0,j\ne m}^{2m}{u^T_{c-m+j}\upsilon_c+2m\log \sum_{k=1}^{\lvert V \rvert} \exp(u^T_k \upsilon_c)}
\end{aligned}$$

&lt;/div&gt;

&lt;h3&gt;Models&lt;/h3&gt;
&lt;p&gt;Minimize $J$ is expensive, as the summation is over $\lvert V \rvert$. There are two ways to reduce the computation. Hierarchical Softmax and Negative Sampling.&lt;/p&gt;
&lt;h4&gt;Hierarchical Softmax&lt;/h4&gt;
&lt;p&gt;Encode words into a huffman tree, then each word has a Huffman code. The probability of it's probability $P(w\lvert Context(\omega))$ can change to choose the right path from root the che leaf node, each node is a binary classification. Suppose code $0$ is a possitive label, $1$ is negative label. If the probability of a possitive classification is 
$$
\sigma(X^T_\omega \theta)=\frac{1}{1+e^{-X^T_\omega}}
$$&lt;/p&gt;
&lt;p&gt;Then the probability of negative classification is
$$
1-\sigma(X^T_\omega \theta)
$$&lt;/p&gt;
&lt;p&gt;&lt;img alt="cbow" src="https://blog.fromkk.com/images/doc2vec_hierarchical_softmax.png"&gt;
足球's Huffman code is $1001$, then it's probability in each node are&lt;/p&gt;
&lt;div&gt;

$$\begin{aligned}
p(d_2^\omega\lvert X_\omega,\theta^\omega_1&amp;amp;=1-\sigma(X^T_\omega \theta^\omega_1))\\
p(d^\omega_3\lvert X_\omega,\theta^\omega_2&amp;amp;=\sigma(X^T_\omega \theta^\omega_2))\\
p(d^\omega_4\lvert X_\omega,\theta^\omega_3&amp;amp;=\sigma(X^T_\omega \theta^\omega_3))\\
p(d^\omega_5\lvert X_\omega,\theta^\omega_4&amp;amp;=1-\sigma(X^T_\omega \theta^\omega_4))\\
\end{aligned}$$

&lt;/div&gt;

&lt;p&gt;where $\theta$ is prarameter in the node.&lt;/p&gt;
&lt;p&gt;The probability of the &lt;code&gt;足球&lt;/code&gt; is the production of these equation.&lt;/p&gt;
&lt;p&gt;Generally,&lt;/p&gt;
&lt;div&gt;

$$p(\omega\lvert Context(\omega))=\prod_{j=2}^{l\omega}p(d^\omega_j\lvert X_\omega,\theta^\omega_{j-1})$$

&lt;/div&gt;

&lt;h4&gt;Negative Sampling&lt;/h4&gt;
&lt;p&gt;Choose some negitive sample, add the probability of the negative word into loss function. Maximize the positive words' probability and minimize the negitive words' probability.&lt;/p&gt;
&lt;p&gt;Let $P(D=0 \lvert \omega,c)$ be the probability that $(\omega,c)$ did not come from the corpus data. Then the objective funtion will be&lt;/p&gt;
&lt;div&gt;

$$\theta = \text{argmax} \prod_{(\omega,c)\in D} P(D=1\lvert \omega,c,\theta) \prod_{(\omega,c)\in \tilde{D}} P(D=0\lvert \omega,c,\theta)$$

&lt;/div&gt;

&lt;p&gt;where $\theta$ is the parameters of the model($\upsilon$ and $u$).&lt;/p&gt;
&lt;p&gt;Ref:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.hankcs.com/nlp/word2vec.html"&gt;word2vec原理推导与代码分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf"&gt;CS 224D: Deep Learning for NLP Lecture Notes: Part I&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/itplus/article/details/37969519"&gt;word2vec 中的数学原理详解（一）目录和前言&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>mathjax</category><category>word2vec</category><guid>https://blog.fromkk.com/posts/models-and-architechtures-in-word2vec/</guid><pubDate>Fri, 05 Jan 2018 13:57:17 GMT</pubDate></item></channel></rss>